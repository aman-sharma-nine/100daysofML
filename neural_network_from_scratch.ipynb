{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural network from scratch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/aman-sharma-nine/100daysofML/blob/master/neural_network_from_scratch.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "nXuL0AG245Eh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYI1qj0N46Op",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network from scratch"
      ]
    },
    {
      "metadata": {
        "id": "entBJCFfAduJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Logical computations with Neurons**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6AUYtPCkAvJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.safaribooksonline.com/library/view/neural-networks-and/9781492037354/assets/mlst_1003.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "0VE8K3DbBNiC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **The Perceptron**\n",
        "The perceptron is the simplist neural network. The input values are assicated with a random weight value *w1* . These weights will later be the parameters to optimize our neural net. The input with its weight value w1 comoutes a weighted sum of its input and bias( z = w1*x1+ w2*x2 ....wn*xn +1) \n",
        "Then sigmoid function is applied to get any values converted between 0 and 1."
      ]
    },
    {
      "metadata": {
        "id": "gsYQ7V_16Ond",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.cs.utexas.edu/~teammco/misc/perceptron/perceptron.png)"
      ]
    },
    {
      "metadata": {
        "id": "gwtU76YDjOwZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Now let's code this in Python using Numpy only"
      ]
    },
    {
      "metadata": {
        "id": "Iy4C65qb4vKN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#installing dependencies \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ie1fCOnk6ODg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Sigmoid Function\n",
        "def sigmoid (x):\n",
        "  return 1/(1+ np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRPlITS35xRU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_values = np.array([[1,0,1],\n",
        "                        [1,1,1],\n",
        "                        [0,1,0],\n",
        "                        [0,1,1],\n",
        "                        [0,0,0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mhW_YJM5obf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#These are the output values used to train out model. T is for transposing to make it 4X1 matrix\n",
        "output_values = np.array([[1,1,0,1,0]]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ajaMN-nnuAY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b27555ad-412d-4f90-d07a-caae7c934a70"
      },
      "cell_type": "code",
      "source": [
        "#create random value for weights \n",
        "np.random.seed(1)\n",
        "weights = 2 * np.random.random((3,1))-1\n",
        "print (\"random weight initilized value: \", weights)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random weight initilized value:  [[-0.16595599]\n",
            " [ 0.44064899]\n",
            " [-0.99977125]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uoOkJizlplBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d094528a-52cb-46f8-8f75-6775d57ffd22"
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(2):\n",
        "  input_layer = input_values\n",
        "  #output values will go through Sigmoid func\n",
        "  output = sigmoid(np.dot(input_values,weights))\n",
        "  \n",
        "print (\"Output after training: \")\n",
        "print (output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output after training: \n",
            "[[0.23762817]\n",
            " [0.3262757 ]\n",
            " [0.60841366]\n",
            " [0.36375058]\n",
            " [0.5       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8QacxSHmvpiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This process of output estimation is called Forward Propogation "
      ]
    },
    {
      "metadata": {
        "id": "zOf1vs14qx9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**NOTE**-  Perceptrons do not output a class probability; rather, they just make predictions based on hard threshold. Thats why most people prefer using Logistic Classifier over Perceptron."
      ]
    },
    {
      "metadata": {
        "id": "0ZurKVNcvpL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will try to optimise values of weights to minimise the error. Error here is the difference between the actual output value in the training set and the predicted value. This process of determining the loss(error) and then propogating it back to the network is called **Backpropogation**\n",
        "The weights are updated iteratively by determing the gradient or derivatives.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2sFPcrgKvJT-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}