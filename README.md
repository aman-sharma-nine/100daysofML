# 100daysofML
100 Days of Machine Learning Coding as proposed by Siraj Raval  
I will keep adding the codes and datasets as I go.  
Day 1:   
Starting [Bloomberg ML Lectures](https://bloomberg.github.io/foml/#lectures)  
Lecture 1- BLACK BOX MACHINE LEARNING - Introducing the standard ML problem types (classification and regression) and discuss prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.  

Day 2:
Started Kaggle competition- https://www.kaggle.com/blastchar/telco-customer-churn
Covered EDA and Preprocessing part of the project. 

Day 3:

Building Xgboost and other ensemble models to evaluate the performance and choose the best scored model.
Hyperparameter tuning [Telco Churn xgboost](https://github.com/aman-sharma-nine/100daysofML/blob/master/xgboost_model.ipynb)

Day 4:
Read Chapter 2: Statistical Learning of ISLR 

Day 5,6,7:
Chapter 8: Tree-Based Methods

Day 8 :
Gradient Boosting

Day 9:
Started Deep Learning Specialization by Andrew Ng from deeplearning.ai  
Completed Week 1 of Neural Networks and Deep Learning Course . 

Day 10:
Week 2
Deep Learning Specialization by Andrew Ng from deeplearning.ai 
Binary Classification and Logistic Regressing using Neural Net

Day 11:

Week 2 
Logistic Reg cost function  
Gardient Descent  
Watched Essence of Calculus - 3Blue1brown (https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) . 

Day 12:  
Started building Neural Network from scratch. 

Day 13:  
Great source to learn how to build single layer and multi layer perceptron and optimizing weight values (Backpropogation)

https://iamtrask.github.io/2015/07/12/basic-python-network/



